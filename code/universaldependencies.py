# -*- coding: utf-8 -*-
"""universaldependencies.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q-eCjO_LgJ3_k3yhxJKfwJFYlZbK6wah
"""

from google.colab import drive
drive.mount('/content/drive')

!mkdir -p ud_data

!wget -P ud_data https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu
!wget -P ud_data https://raw.githubusercontent.com/UniversalDependencies/UD_Finnish-TDT/master/fi_tdt-ud-test.conllu
!wget -P ud_data https://raw.githubusercontent.com/UniversalDependencies/UD_Arabic-PADT/master/ar_padt-ud-test.conllu
!wget -P ud_data https://raw.githubusercontent.com/UniversalDependencies/UD_Chinese-GSD/master/zh_gsd-ud-test.conllu

!pip install stanza

import stanza
import os
from collections import defaultdict
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

stanza.download('en')
stanza.download('fi')
stanza.download('ar')
stanza.download('zh')

test_files = {
    'en': 'ud_data/en_ewt-ud-test.conllu',
    'fi': 'ud_data/fi_tdt-ud-test.conllu',
    'ar': 'ud_data/ar_padt-ud-test.conllu',
    'zh': 'ud_data/zh_gsd-ud-test.conllu'
}

def create_pipeline(lang):
    # Different languages might need different processor combinations
    if lang == 'zh':
        # Chinese might need special handling
        return stanza.Pipeline(lang, processors='tokenize,pos,lemma,depparse',
                              tokenize_no_ssplit=True)
    elif lang == 'ar':
        # Arabic might need MWT (multi-word token) handling
        return stanza.Pipeline(lang, processors='tokenize,mwt,pos,lemma,depparse')
    else:
        # Default configuration for other languages
        return stanza.Pipeline(lang, processors='tokenize,pos,lemma,depparse')

# 3. Functions to load and process CoNLL-U files
def load_conllu(filename):
    sentences = []
    current_sentence = []

    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith('#'):
                if current_sentence:
                    sentences.append(current_sentence)
                    current_sentence = []
            else:
                current_sentence.append(line.split('\t'))

    if current_sentence:
        sentences.append(current_sentence)

    return sentences

def extract_text(sentences):
    texts = []
    for sentence in sentences:
        tokens = []
        for token in sentence:
            if '-' not in token[0] and token[0].isdigit():  # Skip multi-word tokens
                tokens.append(token[1])
        texts.append(' '.join(tokens))
    return texts

# 4. Load gold data for all languages
gold_data = {}
for lang, file_path in test_files.items():
    print(f"Loading gold data for {lang}...")
    gold_sentences = load_conllu(file_path)
    gold_data[lang] = {
        'sentences': gold_sentences,
        'texts': extract_text(gold_sentences)
    }
    print(f"Loaded {len(gold_sentences)} sentences for {lang}")

# 5. Parse a subset of sentences for each language
num_sentences = 50  # Limit for initial development
parsed_data = {}

for lang in test_files.keys():
    print(f"Parsing {lang} sentences...")
    # Use the create_pipeline function
    nlp = create_pipeline(lang)

    # Parse subset of sentences
    parsed_docs = [nlp(text) for text in gold_data[lang]['texts'][:num_sentences]]
    parsed_data[lang] = parsed_docs
    print(f"Parsed {len(parsed_docs)} sentences for {lang}")

# 6. Function to analyze parsing errors
def analyze_parsing_errors(gold_sentences, parsed_docs):
    # Track errors by dependency type
    attachment_errors_by_type = defaultdict(int)
    label_errors_by_type = defaultdict(int)
    total_by_type = defaultdict(int)

    # Track distance of attachment errors
    attachment_error_distances = []

    for gold_sent, parsed_doc in zip(gold_sentences, parsed_docs):
        # Create a mapping from token position to its data
        gold_tokens = {}
        for token in gold_sent:
            if token[0].isdigit() and '-' not in token[0]:  # Skip range tokens
                position = int(token[0])
                head = int(token[6])
                deprel = token[7]
                gold_tokens[position] = (head, deprel)

        # Get the parsed sentence
        parsed_sent = parsed_doc.sentences[0]

        # Compare gold and parsed dependencies
        for word in parsed_sent.words:
            position = word.id

            # Skip if token not in gold (alignment issues)
            if position not in gold_tokens:
                continue

            gold_head, gold_deprel = gold_tokens[position]

            # Count total occurrences by dependency type
            total_by_type[gold_deprel] += 1

            # Check head attachment
            if word.head != gold_head:
                attachment_errors_by_type[gold_deprel] += 1
                # Calculate distance of error
                error_distance = abs(word.head - gold_head)
                attachment_error_distances.append(error_distance)
            elif word.deprel != gold_deprel:
                # Correct head but wrong label
                label_errors_by_type[gold_deprel] += 1

    # Calculate error rates by type
    attachment_error_rates = {dep: errors / total_by_type[dep] if total_by_type[dep] > 0 else 0
                             for dep, errors in attachment_errors_by_type.items()}
    label_error_rates = {dep: errors / total_by_type[dep] if total_by_type[dep] > 0 else 0
                        for dep, errors in label_errors_by_type.items()}

    return {
        'attachment_errors_by_type': dict(attachment_errors_by_type),
        'label_errors_by_type': dict(label_errors_by_type),
        'attachment_error_rates': attachment_error_rates,
        'label_error_rates': label_error_rates,
        'attachment_error_distances': attachment_error_distances,
        'total_by_type': dict(total_by_type)
    }

# 7. Run error analysis for all languages
error_analysis = {}
for lang in test_files.keys():
    print(f"Analyzing errors for {lang}...")
    error_analysis[lang] = analyze_parsing_errors(
        gold_data[lang]['sentences'][:num_sentences],
        parsed_data[lang]
    )
    print(f"Completed error analysis for {lang}")

# common dependency relations
common_deps = set()
for lang, analysis in error_analysis.items():
    # Consider only dependencies with reasonable frequency (at least 10 instances)
    frequent_deps = [dep for dep, count in analysis['total_by_type'].items()
                    if count >= 10]
    if not common_deps:
        common_deps = set(frequent_deps)
    else:
        common_deps = common_deps.intersection(set(frequent_deps))

common_deps = list(common_deps)
print(f"Common dependency relations across languages: {common_deps}")

# Create comparison dataframe for attachment errors
attachment_df = pd.DataFrame({
    lang: [analysis['attachment_error_rates'].get(dep, 0) for dep in common_deps]
    for lang, analysis in error_analysis.items()
}, index=common_deps)

# Create heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(attachment_df, annot=True, cmap='YlOrRd', vmin=0, vmax=1)
plt.title('Attachment Error Rates by Dependency Type Across Languages')
plt.ylabel('Dependency Relation')
plt.tight_layout()
plt.savefig('attachment_errors_heatmap.png')
plt.show()

# Plot error distance distributions
plt.figure(figsize=(10, 6))
for lang, analysis in error_analysis.items():
    distances = analysis['attachment_error_distances']
    if distances:
        sns.kdeplot(distances, label=lang)
plt.title('Distribution of Attachment Error Distances')
plt.xlabel('Distance (tokens)')
plt.ylabel('Density')
plt.legend()
plt.tight_layout()
plt.savefig('error_distance_distribution.png')
plt.show()

problematic_deps = {}

for lang, analysis in error_analysis.items():
    # Create dataframe for error analysis
    error_df = pd.DataFrame({
        'Dependency': list(analysis['attachment_error_rates'].keys()),
        'Error Rate': list(analysis['attachment_error_rates'].values()),
        'Total Count': [analysis['total_by_type'][dep] for dep in analysis['attachment_error_rates'].keys()]
    })

    # Sort by error rate and filter to include only relations with at least 10 instances
    error_df = error_df.sort_values('Error Rate', ascending=False)
    error_df = error_df[error_df['Total Count'] >= 10]

    # Store top 5 problematic dependencies
    problematic_deps[lang] = error_df.head(5)

    print(f"\nTop 5 most problematic dependency relations for {lang}:")
    print(problematic_deps[lang])

# overall error rates
overall_stats = {}

for lang, analysis in error_analysis.items():
    total_deps = sum(analysis['total_by_type'].values())
    total_attachment_errors = sum(analysis['attachment_errors_by_type'].values())
    total_label_errors = sum(analysis['label_errors_by_type'].values())

    overall_stats[lang] = {
        'Attachment Error Rate': total_attachment_errors / total_deps if total_deps > 0 else 0,
        'Label Error Rate': total_label_errors / total_deps if total_deps > 0 else 0,
        'Total Dependencies': total_deps
    }

# Create comparison bar chart
langs = list(overall_stats.keys())
attachment_rates = [overall_stats[lang]['Attachment Error Rate'] for lang in langs]
label_rates = [overall_stats[lang]['Label Error Rate'] for lang in langs]

plt.figure(figsize=(10, 6))
x = np.arange(len(langs))
width = 0.35

plt.bar(x - width/2, attachment_rates, width, label='Attachment Errors')
plt.bar(x + width/2, label_rates, width, label='Label Errors')

plt.xlabel('Language')
plt.ylabel('Error Rate')
plt.title('Overall Parsing Error Rates by Language')
plt.xticks(x, langs)
plt.ylim(0, max(max(attachment_rates), max(label_rates)) * 1.2)
plt.legend()
plt.tight_layout()
plt.savefig('overall_error_rates.png')
plt.show()

phenomena = {
    'coordination': ['conj', 'cc'],
    'subordination': ['advcl', 'acl', 'ccomp', 'xcomp'],
    'nominal_modifiers': ['nmod', 'amod', 'nummod'],
    'function_words': ['case', 'mark', 'aux', 'cop']
}

# Calculate error rates by phenomenon
phenomenon_error_rates = {}
for lang, analysis in error_analysis.items():
    phenomenon_error_rates[lang] = {}

    for phenom, rel_types in phenomena.items():
        total_deps = sum(analysis['total_by_type'].get(rel, 0) for rel in rel_types)
        attachment_errors = sum(analysis['attachment_errors_by_type'].get(rel, 0) for rel in rel_types)

        if total_deps > 0:
            error_rate = attachment_errors / total_deps
        else:
            error_rate = 0

        phenomenon_error_rates[lang][phenom] = {
            'error_rate': error_rate,
            'total_deps': total_deps
        }

# Create bar chart comparing phenomena across languages
phenom_names = list(phenomena.keys())
phenom_df = pd.DataFrame({
    lang: [phenomenon_error_rates[lang][phenom]['error_rate'] for phenom in phenom_names]
    for lang in langs
}, index=phenom_names)

plt.figure(figsize=(12, 6))
phenom_df.plot(kind='bar')
plt.title('Error Rates by Linguistic Phenomenon Across Languages')
plt.xlabel('Phenomenon')
plt.ylabel('Attachment Error Rate')
plt.ylim(0, phenom_df.values.max() * 1.2)
plt.legend(title='Language')
plt.tight_layout()
plt.savefig('phenomenon_error_rates.png')
plt.show()

# Summary of findings
print("\nSUMMARY OF CROSS-LINGUISTIC PARSING ERRORS")
print("============================================")

# Overall performance
print("\nOverall Error Rates:")
for lang in langs:
    print(f"{lang}: Attachment Error Rate = {overall_stats[lang]['Attachment Error Rate']:.4f}, "
          f"Label Error Rate = {overall_stats[lang]['Label Error Rate']:.4f}")

# Most challenging dependency relations
print("\nMost challenging dependency relation for each language:")
for lang in langs:
    if not problematic_deps[lang].empty:
        worst_dep = problematic_deps[lang].iloc[0]
        print(f"{lang}: {worst_dep['Dependency']} (Error Rate: {worst_dep['Error Rate']:.4f})")

# Phenomena comparison
print("\nLinguistic Phenomena Comparison:")
for phenom in phenom_names:
    worst_lang = max(langs, key=lambda l: phenomenon_error_rates[l][phenom]['error_rate'])
    print(f"{phenom}: Most challenging for {worst_lang} "
          f"(Error Rate: {phenomenon_error_rates[worst_lang][phenom]['error_rate']:.4f})")

print("\nAnalysis completed. These findings can form the basis for your UD workshop paper.")

# Save detailed results to CSV files for reference
for lang in test_files.keys():
    pd_data = pd.DataFrame({
        'Dependency': list(error_analysis[lang]['attachment_error_rates'].keys()),
        'Error_Rate': list(error_analysis[lang]['attachment_error_rates'].values()),
        'Count': [error_analysis[lang]['total_by_type'][dep] for dep in error_analysis[lang]['attachment_error_rates'].keys()]
    })
    pd_data.to_csv(f'{lang}_error_analysis.csv', index=False)

# Save your visualizations at higher resolution
plt.figure(figsize=(12, 8), dpi=300)
# ... heatmap code ...
plt.savefig('attachment_errors_heatmap.png', dpi=300, bbox_inches='tight')

# Extract example sentences with coordination errors
def find_examples(gold_sentences, parsed_docs, relation='conj', max_examples=3):
    examples = []
    for i, (gold_sent, parsed_doc) in enumerate(zip(gold_sentences, parsed_docs)):
        # Convert to sentence format for readability
        text = ' '.join([token[1] for token in gold_sent if token[0].isdigit() and '-' not in token[0]])

        # Check if there are errors for the target relation
        has_error = False
        for word in parsed_doc.sentences[0].words:
            position = word.id
            gold_tokens = {int(token[0]): (int(token[6]), token[7])
                          for token in gold_sent if token[0].isdigit() and '-' not in token[0]}

            if position in gold_tokens:
                gold_head, gold_deprel = gold_tokens[position]
                if gold_deprel == relation and word.head != gold_head:
                    has_error = True
                    break

        if has_error:
            examples.append((i, text))
            if len(examples) >= max_examples:
                break

    return examples

# Find examples for coordination errors in each language
coordination_examples = {}
for lang in test_files.keys():
    coordination_examples[lang] = find_examples(
        gold_data[lang]['sentences'][:num_sentences],
        parsed_data[lang],
        relation='conj'
    )
    print(f"\nCoordination error examples for {lang}:")
    for idx, text in coordination_examples[lang]:
        print(f"  Example {idx}: {text}")

!mkdir -p github_files

with open('github_files/README.md', 'w') as f:
  f.write('''# Cross-Linguistic Analysis of Dependency Parsing Errors

This repository contains code and results for analyzing parsing errors across four typologically diverse languages: English, Finnish, Arabic, and Chinese.

## Contents
- `ud_parser_analysis.ipynb`: Jupyter notebook with full analysis code
- `visualizations/`: Folder containing all generated visualizations
- `results/`: CSV files with detailed error statistics by language

## Dependencies
- stanza
- pandas
- matplotlib
- seaborn
- numpy
''')

# Create requirements.txt
with open('github_files/requirements.txt', 'w') as f:
    f.write('''stanza>=1.4.0
pandas>=1.3.0
matplotlib>=3.4.0
seaborn>=0.11.0
numpy>=1.20.0
''')

# Create a directory for results and save detailed CSV files
!mkdir -p github_files/results
for lang in error_analysis.keys():
    pd_data = pd.DataFrame({
        'Dependency': list(error_analysis[lang]['attachment_error_rates'].keys()),
        'Error_Rate': list(error_analysis[lang]['attachment_error_rates'].values()),
        'Count': [error_analysis[lang]['total_by_type'][dep] for dep in error_analysis[lang]['attachment_error_rates'].keys()]
    })
    pd_data.to_csv(f'github_files/results/{lang}_error_analysis.csv', index=False)

# Create a folder for visualizations and save high-resolution versions
!mkdir -p github_files/visualizations

# Save your current visualizations to the GitHub folder
# If you've already created them, you can just copy them
!cp attachment_errors_heatmap.png github_files/visualizations/
!cp error_distance_distribution.png github_files/visualizations/
!cp overall_error_rates.png github_files/visualizations/
!cp phenomenon_error_rates.png github_files/visualizations/

print("GitHub repository files created in 'github_files' directory")